{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - Hadith (Bukhari) Dataset Preprocessing**"
      ],
      "metadata": {
        "id": "OIA1xJxr5A2c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5TdgCQ1Lu9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Hadith Urdu & English Normalization**"
      ],
      "metadata": {
        "id": "OB9yKSLLROgG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UlL-tONIRCAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install camel-tools\n",
        "!pip install pyarabic\n",
        "!pip install clean-text\n",
        "!pip install ftfy\n",
        "!pip install unidecode --quiet\n",
        "\n",
        "import re\n",
        "import json\n",
        "import unicodedata\n",
        "\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "from pyarabic import araby\n",
        "from cleantext import clean\n",
        "from ftfy import fix_text\n"
      ],
      "metadata": {
        "id": "fgl2byAnGINB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = \"/content/6_Al_Hadith_Dataset_FINAL.json\"\n",
        "output_dataset = \"/content/1_Al_Hadith_Dataset_Preprocessed.json\"\n"
      ],
      "metadata": {
        "id": "1ey80KDNRgPh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "URDU_PUNCT_RE = re.compile(r\"[،۔؛؟!,:;]\")\n",
        "\n",
        "DIACRITICS_RE = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n",
        "\n",
        "URDU_MAP = {\n",
        "    \"ي\": \"ی\",\n",
        "    \"ى\": \"ی\",\n",
        "    \"ئ\": \"ی\",\n",
        "    \"ؤ\": \"و\",\n",
        "    \"ك\": \"ک\",\n",
        "    \"ھ\": \"ہ\",\n",
        "    \"ة\": \"ہ\",\n",
        "}\n",
        "\n",
        "# Remove anything not Urdu script, Arabic script, digits, or whitespace..\n",
        "NON_URDU_ALLOWED = re.compile(r\"[^\\u0600-\\u06FF0-9\\s]\")\n",
        "\n",
        "\n",
        "def normalize_urdu(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Unicode normalize\n",
        "    t = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "    # 1) Orthographic replacements\n",
        "    for src, tgt in URDU_MAP.items():\n",
        "        t = t.replace(src, tgt)\n",
        "\n",
        "    # 2) Remove diacritics\n",
        "    t = DIACRITICS_RE.sub(\"\", t)\n",
        "\n",
        "    # 3) Remove Urdu punctuation\n",
        "    t = URDU_PUNCT_RE.sub(\" \", t)\n",
        "\n",
        "    # 4) Remove tatweel\n",
        "    t = t.replace(\"ـ\", \"\")\n",
        "\n",
        "    # 5) Remove non-Urdu characters except whitespace/digits\n",
        "    t = NON_URDU_ALLOWED.sub(\" \", t)\n",
        "\n",
        "    # 6) Collapse spaces\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "\n",
        "    return t\n"
      ],
      "metadata": {
        "id": "ORBm9AYJTYLA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_english(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    t = fix_text(text)\n",
        "    t = clean(\n",
        "        t,\n",
        "        lower=True,\n",
        "        fix_unicode=True,\n",
        "        no_urls=True,\n",
        "        no_punct=True,\n",
        "        no_emoji=True,\n",
        "        no_digits=False,\n",
        "    )\n",
        "\n",
        "    return re.sub(r\"\\s+\", \" \", t).strip()\n"
      ],
      "metadata": {
        "id": "aRF41DSfUCdB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# {\n",
        "#         \"hadith_id\": 1,\n",
        "#         \"source\": \"Hadith\",\n",
        "#         \"book\": \"Sahih Bukhari\",\n",
        "#         \"book_id\": 1,\n",
        "#         \"chapter_id\": 1,\n",
        "#         \"chapter_name_ar\": \"كتاب بدء الوحى\",\n",
        "#         \"chapter_name_ur\": \"وحی کے بیان میں\",\n",
        "#         \"chapter_name_en\": \"Revelation\",\n",
        "#         \"arabic_text\": \"حَدَّثَنَا الْحُمَيْدِيُّ عَبْدُ اللَّهِ بْنُ الزُّبَيْرِ، قَالَ حَدَّثَنَا سُفْيَانُ، قَالَ حَدَّثَنَا يَحْيَى بْنُ سَعِيدٍ الأَنْصَارِيُّ، قَالَ أَخْبَرَنِي مُحَمَّدُ بْنُ إِبْرَاهِيمَ التَّيْمِيُّ، أَنَّهُ سَمِعَ عَلْقَمَةَ بْنَ وَقَّاصٍ اللَّيْثِيَّ، يَقُولُ سَمِعْتُ عُمَرَ بْنَ الْخَطَّابِ ـ رضى الله عنه ـ عَلَى الْمِنْبَرِ قَالَ سَمِعْتُ رَسُولَ اللَّهِ صلى الله عليه وسلم يَقُولُ ‏ \\\"‏ إِنَّمَا الأَعْمَالُ بِالنِّيَّاتِ، وَإِنَّمَا لِكُلِّ امْرِئٍ مَا نَوَى، فَمَنْ كَانَتْ هِجْرَتُهُ إِلَى دُنْيَا يُصِيبُهَا أَوْ إِلَى امْرَأَةٍ يَنْكِحُهَا فَهِجْرَتُهُ إِلَى مَا هَاجَرَ إِلَيْهِ ‏\\\"‏‏.‏\",\n",
        "#         \"urdu_text\": \"آپ صلی اللہ علیہ وسلم فرما رہے تھے کہ تمام اعمال کا دارومدار نیت پر ہے اور ہر عمل کا نتیجہ ہر انسان کو اس کی نیت کے مطابق ہی ملے گا ۔ پس جس کی ہجرت ( ترک وطن ) دولت دنیا حاصل کرنے کے لیے ہو یا کسی عورت سے شادی کی غرض ہو ۔ پس اس کی ہجرت ان ہی چیزوں کے لیے ہو گی جن کے حاصل کرنے کی نیت سے اس نے ہجرت کی ہے ۔\",\n",
        "#         \"english_text\": \"I heard Allah's Messenger (ﷺ) saying, \\\"The reward of deeds depends upon the intentions and every person will get the reward according to what he has intended. So whoever emigrated for worldly benefits or for a woman to marry, his emigration was for what he emigrated for.\\\"\",\n",
        "#         \"narrator\": \"Narrated 'Umar bin Al-Khattab (May Allah be pleased with him):\",\n",
        "#         \"status\": \"Sahih\"\n",
        "#     },"
      ],
      "metadata": {
        "id": "81Qbi8WUIh-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enrich_record(item: dict) -> dict:\n",
        "    return {\n",
        "        \"hadith_id\": item[\"hadith_id\"],\n",
        "        \"chapter_id\": item[\"chapter_id\"],\n",
        "        \"book_id\": item[\"book_id\"],\n",
        "        \"book\": item[\"book\"],\n",
        "        \"source\": item[\"source\"],\n",
        "        \"chapter_name_ar\": item[\"chapter_name_ar\"],\n",
        "        \"chapter_name_ur\": item[\"chapter_name_ur\"],\n",
        "        \"chapter_name_en\": item[\"chapter_name_en\"],\n",
        "        \"narrator\": item[\"narrator\"],\n",
        "         \"status\": item[\"status\"],\n",
        "\n",
        "        \"arabic_text\": item[\"arabic_text\"],\n",
        "        \"normalized_ur\": normalize_urdu(item.get(\"urdu_text\", \"\")),\n",
        "        \"normalized_en\": normalize_english(item.get(\"english_text\", \"\")),\n",
        "    }"
      ],
      "metadata": {
        "id": "fS3JdGtzT4sh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_dataset(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw = f.read().strip()\n",
        "\n",
        "    data = []\n",
        "\n",
        "    if raw.startswith(\"[\"):\n",
        "        data = json.loads(raw)\n",
        "    else:\n",
        "        for line in raw.splitlines():\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "\n",
        "    return [enrich_record(rec) for rec in data]\n",
        "\n",
        "def save_preprocessed_json(records, output_path):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"\\nSaved JSON dataset to {output_path}\")\n",
        "\n",
        "processed_data = load_and_preprocess_dataset(raw_dataset)\n",
        "save_preprocessed_json(processed_data, output_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrUfFKeaU6m6",
        "outputId": "ef9c19e1-bce0-4110-dcaf-d42ca823b634"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved JSON dataset to /content/1_Al_Hadith_Dataset_Preprocessed.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AWNp9w-4UH0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 - Removing Arabic Text**"
      ],
      "metadata": {
        "id": "QLp2JOEvNGO2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDbN7sIGNG_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/1_Al_Hadith_Dataset_Preprocessed.json\"\n",
        "output_path = \"/content/2_Bukhari_Hadith_Preprocessed_FINAL.jsonl\"\n",
        "\n",
        "if input_path.endswith(\".jsonl\"):\n",
        "    data = []\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "else:\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(data)} records\")\n",
        "\n",
        "for item in data:\n",
        "    if \"arabic_text\" in item:\n",
        "        del item[\"arabic_text\"]\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for obj in data:\n",
        "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"\\nSaved JSONL with {len(data)} records ---> {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfFevvSGNHYC",
        "outputId": "76b62c00-faf6-47f9-bb8e-f859d76dc3d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 7563 records\n",
            "\n",
            "Saved JSONL with 7563 records ---> /content/2_Bukhari_Hadith_Preprocessed_FINAL.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSzwBrkBNHTT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}