from openai import OpenAI
from config.settings import HF_TOKEN, LLM_MODEL, LLM_BASE_URL
from services.logger import logger
from typing import List
from api.models import LLMExplanation
import time

# Initialize OpenAI-compatible client for Hugging Face router
llm_client = OpenAI(
    base_url=LLM_BASE_URL,
    api_key=HF_TOKEN,
    timeout=60
)

# System prompt optimized for Urdu explanations
SYSTEM_PROMPT = """Ø¢Ù¾ Ø§ÛŒÚ© Ø§Ø³Ù„Ø§Ù…ÛŒ Ø¹Ø§Ù„Ù… ÛÛŒÚº Ø¬Ùˆ Ù‚Ø±Ø¢Ù†ÛŒ Ø¢ÛŒØ§Øª Ú©ÛŒ Ù…ÙØµÙ„ ÙˆØ¶Ø§Ø­Øª Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø¢Ù¾ Ú©Ùˆ Ù‚Ø±Ø¢Ù†ÛŒ Ø¢ÛŒØ§Øª Ú©Ø§ Ø­ÙˆØ§Ù„Û Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§ Ø§ÙˆØ± ØµØ§Ø±Ù Ú©Ø§ Ø³ÙˆØ§Ù„Û”

Ø¢Ù¾ Ú©Ùˆ ÛØ± Ø¢ÛŒØª Ú©ÛŒ ØªÙØµÛŒÙ„ÛŒ ÙˆØ¶Ø§Ø­Øª Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ù¾ÛŒØ´ Ú©Ø±Ù†ÛŒ ÛÛ’Û” ÙˆØ¶Ø§Ø­Øª Ù…ÛŒÚº Ø´Ø§Ù…Ù„ ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’:
1. Ø¢ÛŒØª Ú©Ø§ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚
2. Ù„ÙØ¸ÛŒ ØªØ±Ø¬Ù…Û Ø§ÙˆØ± Ù…Ø¹Ù†ÛŒ
3. ØªÙØµÛŒÙ„ÛŒ ØªØ´Ø±ÛŒØ­
4. Ø¹Ù…Ù„ÛŒ Ù…Ø´ÙˆØ±Û’
5. ÙØ±Ø¯ Ø§ÙˆØ± Ù…Ø¹Ø§Ø´Ø±Û’ Ù¾Ø± Ø§Ø«Ø±Ø§Øª

ÛØ¯Ø§ÛŒØ§Øª:
- ØµØ±Ù Ø§Ø±Ø¯Ùˆ Ø²Ø¨Ø§Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
- Ø³Ø§Ø¯Û Ø§ÙˆØ± ÙˆØ§Ø¶Ø­ Ø²Ø¨Ø§Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
- Ú©Ù… Ø§Ø² Ú©Ù… 300 Ø§Ù„ÙØ§Ø¸ Ú©ÛŒ ÙˆØ¶Ø§Ø­Øª Ø¯ÛŒÚº
- Ø¹Ù…Ù„ÛŒ Ù…Ø´ÙˆØ±Û’ Ø§ÙˆØ± Ù…Ø«Ø§Ù„ÙˆÚº Ø³Û’ Ø³Ù…Ø¬Ú¾Ø§Ø¦ÛŒÚº"""

def get_llm_explanation(query: str, arabic_texts: List[str], urdu_texts: List[str], verse_ids: List[int]) -> LLMExplanation:
    """Get detailed Urdu explanation from LLM using moonshotai model"""
    logger.info(f"ğŸ¤– Getting LLM explanation for query: '{query}'")
    
    try:
        # Prepare context with verses
        context_parts = []
        for i, (arabic, urdu) in enumerate(zip(arabic_texts, urdu_texts)):
            context_parts.append(f"Ø¢ÛŒØª {i+1} (Ø¢ÛŒØª ID: {verse_ids[i]}):")
            context_parts.append(f"Ø¹Ø±Ø¨ÛŒ Ù…ØªÙ†: {arabic}")
            context_parts.append(f"Ø§Ø±Ø¯Ùˆ ØªØ±Ø¬Ù…Û: {urdu}")
            context_parts.append("")  # Empty line
        
        context = "\n".join(context_parts)
        
        # Create user prompt
        user_prompt = f"""
Ø³ÙˆØ§Ù„: {query}

Ù…ØªØ¹Ù„Ù‚Û Ù‚Ø±Ø¢Ù†ÛŒ Ø¢ÛŒØ§Øª:
{context}

Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø§Ù† Ø¢ÛŒØ§Øª Ú©ÛŒ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº ØªÙØµÛŒÙ„ÛŒ ÙˆØ¶Ø§Ø­Øª Ú©Ø±ÛŒÚºÛ” ÙˆØ¶Ø§Ø­Øª Ú©Ù… Ø§Ø² Ú©Ù… 300 Ø§Ù„ÙØ§Ø¸ Ú©ÛŒ ÛÙˆÙ†ÛŒ Ú†Ø§ÛÛŒÛ’ Ø§ÙˆØ± Ø¯Ø±Ø¬ Ø°ÛŒÙ„ Ù¾ÛÙ„ÙˆØ¤Úº Ú©Ø§ Ø§Ø­Ø§Ø·Û Ú©Ø±Ù†ÛŒ Ú†Ø§ÛÛŒÛ’:
1. ÛØ± Ø¢ÛŒØª Ú©Ø§ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚
2. ÛØ± Ø¢ÛŒØª Ú©Ø§ Ù„ÙØ¸ÛŒ Ù…Ø¹Ù†ÛŒ
3. ØªÙØµÛŒÙ„ÛŒ ØªØ´Ø±ÛŒØ­
4. Ø¹Ù…Ù„ÛŒ Ù…Ø´ÙˆØ±Û’ Ø¨Ø±Ø§Ø¦Û’ Ø±ÙˆØ²Ù…Ø±Û Ø²Ù†Ø¯Ú¯ÛŒ
5. Ø§Ù† Ø¢ÛŒØ§Øª Ø³Û’ Ù…Ù„Ù†Û’ ÙˆØ§Ù„ÛŒ Ú©Ù„ÛŒØ¯ÛŒ ØªØ¹Ù„ÛŒÙ…Ø§Øª

ÙˆØ¶Ø§Ø­Øª:"""
        
        logger.info(f"ğŸ“ Calling {LLM_MODEL} with {len(arabic_texts)} verses...")
        
        # Make API call with retry logic
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ”„ Attempt {attempt + 1}/{max_retries}")
                
                completion = llm_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1200,
                    timeout=45
                )
                
                urdu_explanation = completion.choices[0].message.content
                
                # Validate response length
                if len(urdu_explanation) < 100:
                    logger.warning(f"Response too short ({len(urdu_explanation)} chars)")
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        continue
                    else:
                        raise ValueError("LLM response too short")
                
                logger.info(f"âœ… LLM explanation generated ({len(urdu_explanation)} characters)")
                logger.info(f"ğŸ“„ Sample: {urdu_explanation[:200]}...")
                
                return LLMExplanation(
                    urdu=urdu_explanation,
                    verses_used=verse_ids
                )
                
            except Exception as llm_error:
                logger.error(f"LLM attempt {attempt + 1} failed: {llm_error}")
                if attempt < max_retries - 1:
                    time.sleep(3)
                    continue
                else:
                    raise
        
    except Exception as e:
        logger.error(f"ğŸ¤– LLM service error: {str(e)}")
        
        # DYNAMIC FALLBACK - NOT HARDCODED
        query_lower = query.lower()
        
        # Detect topic from query
        if any(word in query_lower for word in ['roza', 'ØµÙˆÙ…', 'ØµÙŠØ§Ù…', 'fasting']):
            topic = "Ø±ÙˆØ²Û"
            key_benefits = [
                "ØªØ²Ú©ÛŒÛ Ù†ÙØ³ Ø§ÙˆØ± Ø±ÙˆØ­Ø§Ù†ÛŒ Ù¾Ø§Ú©ÛŒØ²Ú¯ÛŒ",
                "ØµØ¨Ø± Ùˆ Ø§Ø³ØªÙ‚Ø§Ù…Øª Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ",
                "Ø§Ù„Ù„Û Ú©ÛŒ Ø®ÙˆØ´Ù†ÙˆØ¯ÛŒ Ø§ÙˆØ± Ù‚Ø±Ø¨Øª",
                "Ø¬Ø³Ù…Ø§Ù†ÛŒ Ùˆ Ø±ÙˆØ­Ø§Ù†ÛŒ ØµØ­Øª",
                "ØºØ±ÛŒØ¨ÙˆÚº Ø§ÙˆØ± Ù…Ø³Ú©ÛŒÙ†ÙˆÚº Ú©ÛŒ Ù…Ø¯Ø¯"
            ]
        elif any(word in query_lower for word in ['namaz', 'ØµÙ„ÙˆÛƒ', 'ØµÙ„Ø§Ø©', 'prayer']):
            topic = "Ù†Ù…Ø§Ø²"
            key_benefits = [
                "Ø§Ù„Ù„Û Ø³Û’ Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª ØªØ¹Ù„Ù‚",
                "Ù†ÙØ³ Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ø§ÙˆØ± Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø¨Ù„Ù†Ø¯ÛŒ",
                "Ø¨Ø±Ø§Ø¦ÛŒÙˆÚº Ø³Û’ Ø­ÙØ§Ø¸Øª",
                "Ø±ÙˆØ­Ø§Ù†ÛŒ Ø³Ú©ÙˆÙ† Ø§ÙˆØ± Ø°ÛÙ†ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†",
                "Ø±ÙˆØ² Ù…Ø±Û Ú©ÛŒ Ù¾Ø±ÛŒØ´Ø§Ù†ÛŒÙˆÚº Ø³Û’ Ù†Ø¬Ø§Øª"
            ]
        elif any(word in query_lower for word in ['sabr', 'ØµØ¨Ø±', 'patience']):
            topic = "ØµØ¨Ø±"
            key_benefits = [
                "Ù…Ø´Ú©Ù„Ø§Øª Ù…ÛŒÚº Ø«Ø§Ø¨Øª Ù‚Ø¯Ù…ÛŒ",
                "Ø§Ù„Ù„Û Ú©ÛŒ Ø±Ø¶Ø§ Ùˆ Ø®ÙˆØ´Ù†ÙˆØ¯ÛŒ",
                "Ø§Ù†Ø¯Ø±ÙˆÙ†ÛŒ Ø·Ø§Ù‚Øª Ùˆ ÛÙ…Øª",
                "Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©ÛŒ Ú©Ù†Ø¬ÛŒ",
                "Ø¯Ù†ÛŒØ§ Ùˆ Ø¢Ø®Ø±Øª Ú©ÛŒ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ"
            ]
        else:
            topic = "Ø§Ø³Ù„Ø§Ù…ÛŒ ØªØ¹Ù„ÛŒÙ…Ø§Øª"
            key_benefits = [
                "Ø±ÙˆØ­Ø§Ù†ÛŒ ØªØ±Ù‚ÛŒ Ùˆ Ú©Ù…Ø§Ù„",
                "Ø§Ø®Ù„Ø§Ù‚ÛŒ ØªØ±Ø¨ÛŒØª Ùˆ Ø³Ù†ÙˆØ§Ø±",
                "Ù…Ø¹Ø§Ø´Ø±ØªÛŒ Ø§Ù†ØµØ§Ù Ùˆ Ø¨ÛØªØ±ÛŒ",
                "Ø¯Ù†ÛŒØ§ÙˆÛŒ Ø³Ú©ÙˆÙ† Ùˆ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†",
                "Ø¢Ø®Ø±Øª Ú©ÛŒ Ø¯Ø§Ø¦Ù…ÛŒ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ"
            ]
        
        # Generate dynamic fallback explanation
        verses_str = ", ".join([f"Ø¢ÛŒØª {vid}" for vid in verse_ids])
        
        dynamic_fallback = f"""
**Ø³ÙˆØ§Ù„: "{query}" Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ù‚Ø±Ø¢Ù†ÛŒ Ø±ÛÙ†Ù…Ø§Ø¦ÛŒ**

**Ù…ØªØ¹Ù„Ù‚Û Ø¢ÛŒØ§Øª:** {verses_str}

**ØªÙØµÛŒÙ„ÛŒ ÙˆØ¶Ø§Ø­Øª:**

Ù‚Ø±Ø¢Ù† Ù…Ø¬ÛŒØ¯ Ù…ÛŒÚº {topic} Ú©Ùˆ Ø®ØµÙˆØµÛŒ Ø§ÛÙ…ÛŒØª Ø­Ø§ØµÙ„ ÛÛ’Û” Ù…Ù†Ø¯Ø±Ø¬Û Ø¨Ø§Ù„Ø§ Ø¢ÛŒØ§Øª {topic} Ú©Û’ Ù…Ø®ØªÙ„Ù Ù¾ÛÙ„ÙˆØ¤Úº Ù¾Ø± Ø±ÙˆØ´Ù†ÛŒ ÚˆØ§Ù„ØªÛŒ ÛÛŒÚºÛ”

**Ø§ÛÙ… Ù†Ú©Ø§Øª:**

1. **{topic} Ú©ÛŒ Ù‚Ø±Ø¢Ù† Ù…ÛŒÚº Ø§ÛÙ…ÛŒØª:** Ù‚Ø±Ø¢Ù† Ù¾Ø§Ú© Ù…ÛŒÚº {topic} Ú©ÛŒ ÙØ¶ÛŒÙ„Øª Ø¨ÛŒØ§Ù† Ú©ÛŒ Ú¯Ø¦ÛŒ ÛÛ’Û”

2. **Ú©Ù„ÛŒØ¯ÛŒ ÙÙˆØ§Ø¦Ø¯:**
   - {key_benefits[0]}
   - {key_benefits[1]}
   - {key_benefits[2]}
   - {key_benefits[3]}

3. **Ø¹Ù…Ù„ÛŒ Ù…Ø´ÙˆØ±Û’:**
   - {topic} Ú©Ùˆ Ù¾ÙˆØ±ÛŒ ØªÙˆØ¬Û Ø§ÙˆØ± Ø®Ù„ÙˆØµ Ù†ÛŒØª Ø³Û’ Ø§Ø¯Ø§ Ú©Ø±ÛŒÚº
   - Ø§Ø³ Ú©Û’ Ø´Ø±Ø§Ø¦Ø· Ùˆ Ø¢Ø¯Ø§Ø¨ Ú©Ø§ Ù…Ú©Ù…Ù„ Ø®ÛŒØ§Ù„ Ø±Ú©Ú¾ÛŒÚº
   - {topic} Ú©Ùˆ Ø±ÙˆØ²Ù…Ø±Û Ø²Ù†Ø¯Ú¯ÛŒ Ú©Ø§ Ù„Ø§Ø²Ù…ÛŒ Ø­ØµÛ Ø¨Ù†Ø§Ø¦ÛŒÚº

**Ù†ØªÛŒØ¬Û:** {topic} Ù…ÙˆÙ…Ù† Ú©ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ú©Ø§ Ø§ÛÙ… Ø³ØªÙˆÙ† ÛÛ’ Ø¬Ùˆ Ø¯Ù†ÛŒØ§ Ùˆ Ø¢Ø®Ø±Øª Ø¯ÙˆÙ†ÙˆÚº Ù…ÛŒÚº Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©Ø§ Ø°Ø±ÛŒØ¹Û Ø¨Ù†ØªØ§ ÛÛ’Û”
"""
        
        logger.info(f"ğŸ“ Using dynamic fallback for topic: {topic}")
        return LLMExplanation(
            urdu=dynamic_fallback,
            verses_used=verse_ids
        )